{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Module 2, Lab 1 - Sampling\n==========================\n\nIn this lab, we will see how random samples (and the data analyses that\ncome from them) estimate the populations they come from.\n\nThis bears repeating: when you are working with a sample of data, you\nare using that as an estimate of the population that generated it.\n\nSo, how good are your estimations? In working with professionals and\nstudents alike, I tend to find that our human intuitions are often\nwrong. However, we can play with sampling ourselves and see the results.\n\nFirst, we should set the seed. A seed is set using the `seed` function from the Numpy.random Python package. This function initializes the random number generation on your computer as mine, so that we should get the same\nresults."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy.random as nr\nnr.seed(12345)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To generate a random sample that is normally distributed, we use the\n`normal(mean, std, n)`. For example, 50 responses from a population with a\nmean of 10 and standard deviation of 2 are:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.normal(10, 2, 50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A Numpy array with the Normally distributed values is returned. \n\nWe can also do something similar with a binomial distribution (data can\nhave two outcomes, such as \"like\" and \"don't like\" a product). Here is\nthe code which uses: `binomial(n, prob, size=1)`. The `prob` argument represents\nthe likelihood of getting a `1` as opposed to a `0`. The size argument\nchanges the nature of the distribution in a way I won't discuss here. If\nwe want to simulate 50 responses from a population in which 30% of\npeople like your product (`1`) and 70% do not (`0`), we use:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(3344)\nnr.binomial(1, 0.3, 50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this case, each `1` represents someone who likes your product and\neach `0` represents someone who does not.\n\nThere are many distributions we can use with many shapes, including\ndistributions that have skew, distributions that can resemble counts of\nthings (e.g., only discrete numbers, most scores zero). We will stick\nwith these two for this lab.\n\n\"Like\" vs \"Dislike\"\n===================\n\nLet's try the example above in which each `1` represents someone who\nlikes your product and each `0` represents someone who does not.\n\nThis time, I will run the sample and save the result."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(3344)\nsample1 = nr.binomial(1, 0.3, 50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we can examine how well our sample did. In this case we *know* the population value was 30%, because we specified that parameter when we ran the code. How close did it get to our true value of 30%? To answer this question execute the `itemfreq` function from the `scipy.stats` module."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from scipy import stats\nprint(stats.itemfreq(sample1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are 13 likes and 37 dislikes. We can convert to\npercentages by diving by the sum of likes and dislikes:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "13.0/(13.0 + 37.0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our sample underestimated the number of people who like the product,\nreturning \"26%\" instead of 30%.\n\nBecause the data are coded `0` and `1`, we can also trick the math into\nreturning a proportion by using `mean()` from Numpy:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nnp.mean(sample1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try this several times. Every time I run the code, a random sample\nwill be collected, the proportion of people who like the product\ncalculated, and reported to you."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(np.mean(nr.binomial(1, 0.3, 50)))\nprint(np.mean(nr.binomial(1, 0.3, 50)))\nprint(np.mean(nr.binomial(1, 0.3, 50)))\nprint(np.mean(nr.binomial(1, 0.3, 50)))\nprint(np.mean(nr.binomial(1, 0.3, 50)))\nprint(np.mean(nr.binomial(1, 0.3, 50)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that our samples are varying quite a bit. We can run many of\nthese by using a list comprehension. Let's try this 100 times. I assume you are\nfamiliar with list comprehensions in Python."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "results = [np.mean(nr.binomial(1, 0.3, 50)) for _ in range(100)]\nprint(results)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see considerable variance in these results. We can histogram them to\nsee it better:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# So the plot appears in line in the noteboook\n%matplotlib inline \n\nsample_mean = np.mean(results)\nimport matplotlib.pyplot as plt\nplt.hist(results)\nplt.vlines(0.3, 0.0, 28.0, color = 'red')\nplt.vlines(sample_mean, 0.0, 28.0, color = 'black')\nplt.xlabel('Results') \nplt.ylabel('Frequency')\nplt.title('Histogram of results')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that, on average, random samples are trustworthy--after all,\nthey are tending toward 30%. However, *individual* samples are\nless trustworthy. Some results are nearly as large as 135% or as low as\n50%. Yikes!\n\nWe can also subtract .30 from each score to re-score them as the degree\nof error in each sample."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "results_error = [round(x - 0.3, 2) for x in results]\nprint(results_error)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that most sample scores are within about 5% of the true\npopulation value value. Still, depending on what we want to do with the\ndata, that could be unacceptably large. The property of samples to\n\"mis-estimate\" the population is called sampling error and it is clearly\na big problem, leading to many a bad decision. The degree to which your\nindividual samples tend to \"mis-estimate\" the population (shown above:\n`results_error`) is something we want to estimate. Typically, we\nquantify this by taking the standard deviation of these errors. This is\ncalled \"standard error\", and it is a single number, how far \"off\" our\nsamples tend to be, on average:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.std(results_error)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Aha, so we see that the average sample is \"off\" from the population\nvalue by 6%. Some are \"off\" by more; some are \"off\" by less, but the\naverage sample is off by 6%. In other words, our standard error is 6%.\n\nFun fact: you can also estimate the standard error with a simple\nequation. For binomial data (`0` and `1` scores), the equation is:\n\n$$se = \\sqrt{\\frac{p\\left ( 1-p \\right )}{n-1}}$$\n Here, p is the percentage in the population. So, plugging in our\nvalues:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import math\nmath.sqrt((.30*(1-.3))/(50-1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is convenient, because it tells us that we don't really need to run\nsimulations like the above to know how trustworthy our samples are. In\nfact, plugging in a reasonable guess for the population value and a\nsample size, we can know *before we run a study* how trustworthy a\ntypical sample will be.\n\nClearly, a large standard error is a bad thing. We can reduce this\nproblem by relying on a larger sample. For example, try using a sample\nof 700 in the equation for standard error shown previously:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "math.sqrt((.30*(1-.3))/(700-1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see now that the typical sample will be off by only 1.7% from the\npopulation value. We can run a similar loop as done before and see this\nin action:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "results = [np.mean(nr.binomial(1, 0.3, 700)) for _ in range(100)]\n\nprint(np.std(results))\n\nsample_mean = np.mean(results)\nimport matplotlib.pyplot as plt\nplt.hist(results)\nplt.vlines(0.3, 0.0, 28.0, color = 'red')\nplt.vlines(sample_mean, 0.0, 28.0, color = 'black')\nplt.xlabel('Results') \nplt.ylabel('Frequency')\nplt.title('Histogram of results')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here, now that most results between 82% and 112%, with the typical\nresult being \"off\" by only 1.7%...exactly as our standard error equation\npredicted.\n\nEvery data situation has a standard error. The point is not to learn a\nlarge number of equations but rather to emphasize the following point:\nsamples (and the statistics they produce) are flawed estimates of the\npopulation. However, they become more and more accurate as the sample\nsizes they are based on increase.\n\nWe will discover, soon, that this will give us the concept of\nstatistical power. Large samples will produce results strong enough that\nwe can make meaningful statements about the population (in such\nsituations, we have \"good power\"), where small samples contain so much\nerror that we cannot say much meaningful about the population (\"weak\npower\")."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}