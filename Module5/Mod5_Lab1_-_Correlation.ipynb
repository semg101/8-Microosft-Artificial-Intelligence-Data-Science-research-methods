{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 5, Lab 1 - Correlation\n",
    "=============================\n",
    "\n",
    "This final series of labs explores everything you need to execute\n",
    "projects from start to finish based on a few different analyses.\n",
    "\n",
    "In this lab, we will explore how to assess relationships between\n",
    "variables using correlation in R.\n",
    "\n",
    "In this example, we have a dataset, inspired by a dataset published on\n",
    "kaggle (<https://www.kaggle.com/unsdsn/world-happiness>). In this\n",
    "dataset, several regions of the world are compared on dimensions such as\n",
    "their generosity, happiness, GDP, and so forth.\n",
    "\n",
    "Load Packages\n",
    "=============\n",
    "\n",
    "In this lab, we will use the `ggplot2` package for data visualization,\n",
    "the `corrplot` package for making visual correlation tables, and the\n",
    "`psych` package for detecting skew and making correlation matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "=========\n",
    "\n",
    "Next, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATA ####\n",
    "dat = pd. read_csv(\"datasets/regionalhappy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the dataset, we see the names are a little messy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rename them easily by assigning names to the `columns` attribute of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.columns = [\"Happiness\", \"GDP\", \"Family\", \"Life_Expect\", \"Freedom\", \"Generosity\", \"Trust_Gov\", \"Dystopia\"]\n",
    "dat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better.\n",
    "\n",
    "Bivariate Correlation\n",
    "=====================\n",
    "\n",
    "Next, let's see how variables correlate. In our research study, we want\n",
    "to understand happiness. We can compute correlations between variables\n",
    "with the Pandas `corr()` method. The upper or lower off-diagonal elements of the 2X2 correlation matrix contain the correlation coefficients. The Pandas `iloc` method allows you to address these elements using numerical indicies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = dat[['Happiness', 'Life_Expect']].corr()\n",
    "corr_mat.iloc[1,0].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the correlation is *r* = .78. A brief refresher: correlations\n",
    "range between zero (no association between variables) and 1.0 (a\n",
    "one-to-one association). They can also be positive (as one variable\n",
    "increases, so does the other) or negative (as one variable increases,\n",
    "the other decreases).\n",
    "\n",
    "So, in this case, we have a large, positive link between the happiness\n",
    "of a region and health / life expectancy in that region. The\n",
    "statistician Jacob Cohen suggested the following guidelines:\n",
    "\n",
    "|        | Correlation          | Meaning  |\n",
    "| ------------- |:-------------:|-----------:|\n",
    "| 1.  | 0 - 0.1 | Negligible |\n",
    "| 2.  | 0.1 - 0.3     |  Small |\n",
    "| 3. | 0.3 - 0.5      |  Medium |\n",
    "| 4. | 0.50 +      |  Large |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, then, that this association would count as \"large\" by Cohen's\n",
    "guidelines.\n",
    "\n",
    "We can also easily visualize this correlation with `ggplot2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8, 8)).gca() # define axis\n",
    "dat.plot.scatter(x = 'Happiness', y = 'Life_Expect', ax = ax, alpha = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### A Sample Estimate\n",
    "\n",
    "We have discussed statistical hypothesis testing a number of times in\n",
    "this course, but we haven't yet discussed it in detail with respect to\n",
    "correlations. So, here is a brief refresher on the need for significance\n",
    "testing, applied to correlation.\n",
    "\n",
    "Here, are working with a sample of regions at one point in time. What if\n",
    "we wanted to estimate, in a broader way, the association between\n",
    "happiness and life expectancy? Assuming our data are representative of\n",
    "the broader population (e.g., across times, regions, etc.; a big\n",
    "assumption!), we could use this sample correlation (symbol: *r* = .78)\n",
    "as an estimate of the population correlation (symbol: *ρ*). In other\n",
    "words, we don't know the true correlation between happiness and life\n",
    "expectancy in the population, but if we can trust this data to\n",
    "adequately represent it, we can *estimate* it at .78.\n",
    "\n",
    "The estimation piece is important. Often, people look at the sample\n",
    "correlation and don't realize that it's specific to that sample. For\n",
    "example, an organization might collect a survey to assess the link\n",
    "between customer satisfaction and consumption. Whatever correlation\n",
    "observed in the sample is only an estimate--our best guess--of the\n",
    "correlation in the broader population. Were that organization to collect\n",
    "another sample, they would get a different correlation. Every time, the\n",
    "correlation would vary slightly, because the sample is different and\n",
    "only representing (but not being) the population. This raises an\n",
    "important point: sample correlations are imperfect estimates of their\n",
    "population counterparts. The sample estimate has error built into it.\n",
    "\n",
    "One important consequence is that it is possible that the correlation in\n",
    "the population is actually zero (*H*<sub>0</sub> : *ρ* = 0) even when it\n",
    "is not in the sample (e.g., *r* = .12). In other words, the sample\n",
    "correlation could be a statistical fluke of the sample. We cannot say,\n",
    "just because the sample correlation is nonzero, that the two variables\n",
    "truly are correlated in the population. We will need to conduct a\n",
    "statistical significance test first.\n",
    "\n",
    "Further, we can *only* trust the sample correlation as an estimate of\n",
    "the population correlation *if* the data are representative. If only a\n",
    "certain kind of person selects into the survey (e.g., a certain\n",
    "personality type, people who have strong feelings about a product,\n",
    "etc.), then the sample correlation will estimate the correlation for\n",
    "*that population only.* This essentially means that all research data is\n",
    "biased toward whoever is over-represented in the sample. For this\n",
    "reason, getting good information on whoever is represented in one's data\n",
    "is very important for qualifying the results. In some cases, it may be\n",
    "worth it to gather data from multiple different sources or methods and\n",
    "cross-reference the results for very important decisions.\n",
    "\n",
    "Significance Test\n",
    "=================\n",
    "\n",
    "If we want to test the correlation for significance, we can use the 'pearsonr' function from the scipy.stats modules. The confidence intervals can be computed using the methods in the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_z(r): ## transform distribution\n",
    "    return math.log((1 + r) / (1 - r)) / 2.0\n",
    "\n",
    "def z_r(z): ## inverse transform distribution \n",
    "    e = math.exp(2 * z)\n",
    "    return((e - 1) / (e + 1))\n",
    "\n",
    "def r_conf_int(r, alpha, n):\n",
    "    # Transform r to z space\n",
    "    z = r_z(r)\n",
    "    # Compute standard error and critcal value in z\n",
    "    se = 1.0 / math.sqrt(n - 3)\n",
    "    z_crit = ss.norm.ppf(1 - alpha/2)\n",
    "\n",
    "    ## Compute CIs with transform to r\n",
    "    lo = z_r(z - z_crit * se)\n",
    "    hi = z_r(z + z_crit * se)\n",
    "    return (lo, hi)\n",
    "\n",
    "def correlation_sig(df, col1, col2):\n",
    "    pearson_cor = ss.pearsonr(x = df[col1], y = dat[col2])\n",
    "    conf_ints = r_conf_int(pearson_cor[0], 0.05, 1000)\n",
    "    print('Correlation = %4.3f with CI of %4.3f to %4.3f and p_value %4.3e' \n",
    "        % (pearson_cor[0], conf_ints[0], conf_ints[1], pearson_cor[1]))\n",
    "    \n",
    "correlation_sig(dat, 'Happiness', 'Life_Expect')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence interval is fairly narrow around the correlation coefficient. \n",
    "\n",
    "Using the same information, we can make a reasonable guess about what\n",
    "the correlation in the population is. We see that `cor.test()` has given\n",
    "us a 95% confidence interval of \\[0.71, 0.84\\], meaning that we are 95%\n",
    "confident that the population value (*ρ*) is in that range. By \"95%\n",
    "confident,\" we mean that this range includes the population value 95% of\n",
    "the time. If we act on it and trust it, we are right 95% of the time.\n",
    "\n",
    "So, we are pretty certain that, even though we have a sample (a small\n",
    "sample, too!), that there is a large correlation in the population\n",
    "between happiness and life satisfaction. Even after taking the\n",
    "uncertainty of our sample into account (e.g., with the *t*-test and 95%\n",
    "CI), we still feel confident that there is a larger link between these\n",
    "two variables.\n",
    "\n",
    "Caveat: Normality\n",
    "=================\n",
    "\n",
    "It should be noted that correlations work best with normally distributed\n",
    "(bell curve, symmetrical) data. We can briefly check the skew of the\n",
    "variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8, 6)).gca() # define axis\n",
    "dat.Happiness.plot.hist(ax = ax, alpha = 0.4)\n",
    "plt.title('Histogram of Happiness')\n",
    "plt.xlabel('Happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8, 6)).gca() # define axis\n",
    "dat.Life_Expect.plot.hist(ax = ax, alpha = 0.4)\n",
    "plt.title('Histogram of Life Expectancy')\n",
    "plt.xlabel('Life Expectancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We see here that both variables are decently normally distributed, but\n",
    "life expectancy is possibly negative skewed (i.e., \"skew left\"). We can\n",
    "get a metric of the skew using the `skewtest()` function from the Scipy.Stats\n",
    "package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = ss.skewtest(dat.Happiness)\n",
    "skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People disagree about what is acceptable skew, but usually a value less\n",
    "than +/- 1.0 raises no alarms. Further, the p-value is rather large indicating we cannot reject the null hypothesis that there is no skew. Here, we can safely go about our\n",
    "business.\n",
    "\n",
    "However, if we had a bigger skew problem, we could also address the skew\n",
    "by transforming the variable. (There are also backup options, such as\n",
    "Spearman's correlation, but we won't explore that in this class beyond\n",
    "what was covered in Module 3).\n",
    "\n",
    "Correlations are based on variance, so anything that biases a mean\n",
    "(e.g., skew) also interferes with the correlation. In general, skew\n",
    "reduces correlations. For a more robust test of the correlation, you can\n",
    "transform the data by performing a mathematical operation to every\n",
    "score. There are many such operations we can try. In general, taking the\n",
    "square root of every score reduces skew, but the catch is that the\n",
    "variable must be positively skewed and no scores may be negative. For\n",
    "the sake of illustration, let's try this.\n",
    "\n",
    "First, let's \"reverse\" the variable. Take the maximum score, add one to\n",
    "it, and subtract your score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['Life_Expect2']= max(dat.Life_Expect) + 1 - dat.Life_Expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reverses the variable as shown in the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8, 6)).gca() # define axis\n",
    "dat.Life_Expect2.plot.hist(ax = ax, alpha = 0.4)\n",
    "plt.title('Histogram of reversed Life Expectancy')\n",
    "plt.xlabel('Reversed Life Expectancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod3_Lab1_-_Correlation_files/figure-markdown_strict/unnamed-chunk-13-1.png)\n",
    "\n",
    "Now we can perform any number of operations. The square root is the most\n",
    "mild transformation. We can also take the natural log of every score (no\n",
    "values may be zero!). In general, these operations reduce big numbers\n",
    "more than small numbers and thus rein in the long tail. The Pandas `apply` method is used to apply the `math.sqrt` function to each element of `Life_Expect2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Square the reversed variable\n",
    "dat['Life_Expect2_sqrt'] = dat.Life_Expect2.apply(math.sqrt)\n",
    "\n",
    "ax = plt.figure(figsize=(8, 6)).gca() # define axis\n",
    "dat.Life_Expect2_sqrt.plot.hist(ax = ax, alpha = 0.4)\n",
    "plt.title('Histogram of reversed Life Expectancy squared')\n",
    "plt.xlabel('Reversed Life Expectancy squared')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod3_Lab1_-_Correlation_files/figure-markdown_strict/unnamed-chunk-14-1.png)\n",
    "We could save this transformed version, then re-reverse it and use that\n",
    "in subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## un-reverse the variable\n",
    "dat['Life_Expect2']= max(dat.Life_Expect2_sqrt) + 1 - dat.Life_Expect2_sqrt\n",
    "\n",
    "## compute correlation of new variable and test statistics\n",
    "correlation_sig(dat, 'Happiness', 'Life_Expect2')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here the results barely changed. In this case, that wasn't really\n",
    "necessary because the variable was not that skewed to begin with. You\n",
    "will find, in many cases, that it is very helpful, however. In those\n",
    "cases, there are many online guides to data transformation.\n",
    "\n",
    "Of course, if you are predicting something that has a very non-normal\n",
    "distribution (e.g., categorical variables, counts of things often have\n",
    "many zeros, etc.) then correlation may not be the best tool to use. A\n",
    "more sophisticated data modeling technique may be warranted. However,\n",
    "\n",
    "Correlations Among Many Variables\n",
    "=================================\n",
    "\n",
    "Often we want to examine the correlations among many variables at once.\n",
    "In this case, we could look at a matrix of correlations. This can be\n",
    "done using the `corr()` method on a pandas data frame. You can use the entire data frame of subset it. In this case we just want the correlations between happiness, life\n",
    "expectancy, GDP, and generosity. Notice the double `[[]]` the outer `[]` is the subset operator and the inner `[]` delimits the list of column names:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see that happiness, life expectancy, and GDP are all\n",
    "highly intercorrelated, whereas generosity is seemingly less related.\n",
    "Thus, we may conceive that we have a \"cluster\" of intercorrelated\n",
    "variables around happiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[['Happiness', 'Life_Expect', 'Generosity']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that generosity is not significantly correlated with the other\n",
    "variables.\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "We saw before we had a cluster of several overlapping variables. We can\n",
    "make some helpful visuals to view this cluster. The code in the cell below performs the following processing:\n",
    "1. The correlation matrix is computed using the Pandas `corr` method and with two unneeded columns dropped from the data frame. \n",
    "2. A hierarchical clustering model is constructed using methods from the scipy.cluster.hierarchy package. A hierarchical cluster model is created by linking (using a `linkage` function) values based on a measure of distance into a hierarchy or tree. The details of hierarchical clustering are beyond the scope of this course. \n",
    "3. The correlation matrix is permuted based on the hierarchy and plotted as a heat map. \n",
    "\n",
    "Execute the code below and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the correlation matrix\n",
    "corrs = dat.drop(['Life_Expect2', 'Life_Expect2_sqrt'], axis = 1).corr()\n",
    "\n",
    "## Create the hierarchical clustering model\n",
    "dist = sch.distance.pdist(corrs)   # vector of pairwise distances using correlations\n",
    "linkage = sch.linkage(dist, method='complete') # Compute the linkages for the clusters\n",
    "ind = sch.fcluster(linkage, 0.5*dist.max(), 'distance')  # Apply the clustering algorithm\n",
    "\n",
    "## Order the columns of the correlaton matrix according to the hierarchy\n",
    "columns = [corrs.columns.tolist()[i] for i in list((np.argsort(ind)))]  # Order the names for the result\n",
    "corrs_clustered = corrs.reindex(columns) ## Reindex the columns following the heirarchy \n",
    "\n",
    "## Plot a heat map of the clustered correlations\n",
    "sns.heatmap(corrs_clustered, \n",
    "            xticklabels=corrs_clustered.columns.values,\n",
    "            yticklabels=corrs_clustered.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see there is a prominent cluster of happiness, family, life\n",
    "expectancy, and GDP. These are all highly inter-correlated. You can see\n",
    "this from the bright spot in the heat map.\n",
    "\n",
    "#### Another Correlation Plot\n",
    "\n",
    "With small number variables, it can be useful to display the correlation matrix with the numerical values. The Pandas `style.background_gradient()` does just this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_clustered.style.background_gradient().set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The darker the color, the higher the correlation. The lightest colors are negative correlations. \n",
    "\n",
    "Notice that several variables exhibit correlations quite close to 0. It is a good idea to see if these correlations are even significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod3_Lab1_-_Correlation_files/figure-markdown_strict/unnamed-chunk-20-1.png)\n",
    "\n",
    "We can also add a grid of *p*-values with `p.mat=` and tell the function\n",
    "to \"X\" out anything not &lt; .05 by adding `sig.level = .05`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_sig2(df, col1, col2):\n",
    "    pearson_cor = ss.pearsonr(x = df[col1], y = dat[col2]) \n",
    "    conf_ints = r_conf_int(pearson_cor[0], 0.05, 1000)\n",
    "    print('Correlation with ' + col2 + ' = %4.3f with CI of %4.3f to %4.3f and p_value %4.3e' \n",
    "        % (pearson_cor[0], conf_ints[0], conf_ints[1], pearson_cor[1]))\n",
    "\n",
    "def test_significance(df, col_list):\n",
    "    cols = df.columns\n",
    "    for col1 in col_list: \n",
    "        print('\\n')\n",
    "        print('Significance of correlations with ' + col1)\n",
    "        for col2 in cols: \n",
    "            if(col1 != col2):\n",
    "                correlation_sig2(df, col1, col2)  \n",
    "\n",
    "test_cols = ['Trust_Gov', 'Generosity', 'Dystopia']\n",
    "test_significance(dat, test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most the the correlations with Trust_Gov are significant. However, most correlations with Generosity and Dystopia are not. The lack of significance can been seen by confidence intervals that straddle 0.0 and have large p-values. \n",
    "\n",
    "There you have it. There are many great ways to illustrate correlations\n",
    "among data.\n",
    "\n",
    "Conclusion\n",
    "==========\n",
    "\n",
    "Using the correlation analysis, we have both learned to find clusters of\n",
    "relationships among data and to estimate individual correlations and\n",
    "test them for significance. If we had a specific variable we wanted to\n",
    "study in greater detail, we could graduate to regression, which we will\n",
    "do in the next lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
